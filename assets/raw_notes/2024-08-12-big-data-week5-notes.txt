---

layout: post  
title: "CSCI946 - Big Data Analytics Week 5"  
date: 2024-08-19 15:09:00  
description: "classification"  
tags: projects learning uow
categories: learning  
giscus_comments: true  
featured: false  

---

### Previous Week Clusterin

Clustering is Unsupervised

K mean clutering
finding centers where data converges

variation = dog and rabbit 
application - can be used for image compression/processing


DBSCAN
Density-based clustering 
core point, border point, noise point

Self Organizing Map
Neural Network like approach (each neuron can be also assumed as center in kmean)
High dimension into low dimension (2d)

it preserve topology (unlike PCA)

### Classification

Classification is supervised

Examples
K nearest neighbors (KNN)
Neural Network or MLP (Multi Layer Precepton)
Decision Tree and random Forest
Naive Bias
Logistic regression
Support vector Machines (SVM)

### K nearest neighbor

Require
Label dataset 

mostly use odd number (even number has descision problem (tie))
distance between two points - Euclidean distance

Weight the vote acc to distance w = 1/d^2


choosing k
k is too small - sensitive to noise
k is too large - other classes may include
Computation cost increases with K (use sorting)

k-d tree


### MLP

Use neuron 

Neuron Summmary
neuron -> Recieve Input -> Weighted Sum -> Activate (Based on activation function)


For MLP output is two


Design
common - three layers for Linear activation, two layers for non Linear

Weight update
Gradient Descent (loss) and Back propogation

Caution
black boxes,
training can take time,
overfit,
problem with unbalanced dataset

Good
insenstive to noise
non linear mapping
universal function approximator

Descision Trees
A tree structure to split dataset

Types of Node
Root -> Branch -> Internal Node -> Leaf Node

Buiding Tree
The most informative attribute is defined by Entropy - Information Gain 

Entropy

Hx = - Sum for all X (P(x) log2(Px))


Conditional Entropy
H Y|X

Info Gain

HS - HS|A

- The degree of purity of the parent node before a split 
– The degree of purity of the child node after a split


Tree can split in more than two (M way tree)


The algo splits on the attribute with largest information Gain

Good
It is explainable
Computation inexpensive
Can handle both numerical and categorical
not good choice if there are many irrevalent features


Caution
They are greedy algo (chooses best at the available moment)

An ensemble technique can handle this (ex random forest)

### Naive Bayes

Probabilty based theorem based on Bayes theorem


P (C|A) = P(A|C)* P(C) / P(A)

Hard to calculate if attributes are dependent


First
 Conditional independence assumption

 Each attribute is conditionally independent of 
every other attribute given a class label ci

Second, ignore the denominator P(A)
– Removing the denominator has no impact on the 
relative probability scores


log is used to make calc stable (log can make number not go too small)

Caution
Rare event - if something is not sample or not accounted in probability

Smoothing technique

It assigns a small nonzero probability to rare
events not included in a training dataset

Good

Simple to implment
can handle high dim data
not likely to overfit

### Peformance Indicators


- Holdout (80-20, 70-30 split)

- Cross Validation 

Divide dataset into k chunks


Stratified cross-validation

Stratified - class dist are equal

folds are stratified so that 
class dist. in each fold is approx. the same as that in the 
initial data



Confusion matrix

Accuracy = (TP + TN)/All
Error rate = (FP + FN)/All


Sensitivity = TP/P
Specificity = TN/N

 Precision: exactness – what % that the classifier labeled as 
positive are actually positive

Recall: completeness – what % of the positives did the 
classifier label as positive? (equals to sensitivity)

F measure (F1 or F-score): harmonic mean of precision and 
recall


Check 3 blue1brown bayes video once again




















