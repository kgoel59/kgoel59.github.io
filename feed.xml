<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://karangoel59.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://karangoel59.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-12T12:21:09+10:00</updated><id>https://karangoel59.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">CSCI971 - Modern Cryptography Week 3</title><link href="https://karangoel59.com/blog/2024/crypto-week3/" rel="alternate" type="text/html" title="CSCI971 - Modern Cryptography Week 3"/><published>2024-08-10T01:09:00+10:00</published><updated>2024-08-10T01:09:00+10:00</updated><id>https://karangoel59.com/blog/2024/crypto-week3</id><content type="html" xml:base="https://karangoel59.com/blog/2024/crypto-week3/"><![CDATA[<p>How to construct symmetric key encription</p> <p>We will use computer complexity and check if scheme is secure</p> <p>Roadmap Classic Cipher = Ceaser Cipher (algo based) One time pad = Shannon</p> <p>How to design a block cipher</p> <p>Confusion and Diffusion: Design Principles</p> <p>Confusion = if a single bit key is changed, cypher text is significantly different Diffusion = if a single bit in plaintext is changed, cypher text is significantly different</p> <p>Change in plain text and and key result significant change in cypher text</p> <p>This is called avalanche effect</p> <p>Block Cipher Message -&gt; Enc(key) -&gt; Cipher -&gt; Dec(key) -&gt; Message</p> <p>Typical block size 64 bits 128 bits</p> <p>Common Design Approaches iterated cipher =&gt; each iteration is called round. The output of each round is input to next round</p> <p>DES = 16 rounds AES = 10 rounds</p> <p>Each Round inside AES is: Subsitution Permutation network</p> <p>DES (Data Encryption Standard)</p> <p>why it is developed 1970 need standard encryption scheme</p> <p>Standard need to have following properties</p> <ul> <li>high level security</li> <li>accordance with Kirchoff’s law</li> <li>economic</li> <li>adaptable</li> </ul> <p>DES (64 bit input, 64 bit output, 56 bit key)</p> <p>16 round Feistel Network</p> <p>Feistel Network</p> <p>introduced by Feistel</p> <p>how it work</p> <p>original input - divide into two part L0- 32 bit R0 - 32 bit</p> <p>(check week 3 slides) R1 = R0 -&gt; F(K) -&gt; XOR L0 L1 = R0</p> <p>Last Round</p> <p>LN = R(N-1) RN = R(N-1) -&gt; F(K) -&gt; XOR L(N-1)</p> <p>S-box</p> <p>An S-box is a basic component in symmetric key algorithms, used to perform substitution. It takes an input of a fixed size (a string of bits) and transforms it into an output of a fixed size. This substitution process is nonlinear, which is essential for the cryptographic strength of the algorithm</p> <p>Des shortcomming 2^56 keys = 10^17 keys (very short range keys)</p> <p>we can test keys in parallel</p> <p>3DES</p> <p>AES (Good enough for today) (I think there was a video in youtube) Does not use Feistel</p> <p>Is a blockcipher a secure symmetric-key encryption NO</p> <p>it preserve statistacilly property of plain text (if you use it directly)</p> <p>cypher text space should be much larger than plain text</p> <p>Solution Randomize Choose ramdom IV M’ = IV XoR M C &lt;- Enc(K,M’)</p> <p>We say that the encryption is secure if no P.P.T adversary can win with a probability of ½+1/poly(λ)</p> <p>The underlying blockcipher is assumed to be a pseudorandom function (PRF), i.e., outputs of the blockcipher is indistinguishable from random values.</p> <p>Encrypt long message</p> <p>Electronic Code Book Divide in block run cypher</p> <p>CBC (Cypher block chain) Initialize vector</p> <p>CFB (Cypher Feedback ) Cypher of one block use as input to other not IV</p> <p>Output feedback mode (OFB)</p> <p><a href="/assets/pdf/crypto/3.%20CSCI471971%20Symmetric-key%20Encryption.pdf">Lecture 3</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[Block Cyphers and Symetric Key Encription"]]></summary></entry><entry><title type="html">CSCI946 - Big Data Analytics Week 3</title><link href="https://karangoel59.com/blog/2024/big-data-week3/" rel="alternate" type="text/html" title="CSCI946 - Big Data Analytics Week 3"/><published>2024-08-06T01:09:00+10:00</published><updated>2024-08-06T01:09:00+10:00</updated><id>https://karangoel59.com/blog/2024/big-data-week3</id><content type="html" xml:base="https://karangoel59.com/blog/2024/big-data-week3/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Data exploration is a critical stage in the data analytics lifecycle. It involves understanding and preparing data to facilitate effective analysis and model building. Here’s a streamlined overview of the key stages and techniques involved.</p> <h3 id="data-analytics-lifecycle">Data Analytics Lifecycle</h3> <p><strong>1. Discovery</strong></p> <ul> <li><strong>Learn Business Domain</strong>: Understand the industry and context.</li> <li><strong>Interview Sponsor &amp; Identify Stakeholders</strong>: Engage with key individuals to gather insights.</li> <li><strong>Define Resources &amp; Goals</strong>: Establish objectives and available resources.</li> <li><strong>Identify Potential Data Sources</strong>: Locate relevant data sources.</li> <li><strong>Frame the Problem &amp; Develop Initial Hypotheses</strong>: Formulate hypotheses, including Null Hypothesis (H0) and Alternative Hypothesis (HA or H1).</li> </ul> <p><strong>2. Data Preparation</strong></p> <ul> <li><strong>Prepare Sandbox</strong>: Set up an environment for data preparation.</li> <li><strong>Perform ETLT (Extract, Transform, Load, Transform)</strong>: Process data for analysis.</li> <li><strong>Understand Data Details</strong>: Examine the data’s structure and quality.</li> <li><strong>Data Conditioning</strong>: Address issues like missing values and outliers.</li> <li><strong>Format Data</strong>: Prepare data for analysis.</li> <li><strong>Visualize Data</strong>: Use plots to explore data patterns.</li> </ul> <p><strong>3. Model Planning</strong></p> <ul> <li><strong>Select Variables</strong>: Based on relationships (e.g., correlation matrix) and domain knowledge.</li> <li><strong>Identify Candidate Models</strong>: Refer to hypotheses, translate into machine learning models, review literature, and document assumptions.</li> </ul> <p><strong>4. Model Building</strong></p> <ul> <li><strong>Create Datasets</strong>: Prepare training, validation, and testing datasets.</li> <li><strong>Train and Test Models</strong>: Evaluate model performance.</li> </ul> <p><strong>5. Communicating Results</strong></p> <ul> <li><strong>Compare Results</strong>: Assess against criteria.</li> <li><strong>Articulate Findings</strong>: Clearly present results.</li> <li><strong>Discuss Limitations &amp; Recommendations</strong>: Provide insights on limitations and suggest improvements.</li> </ul> <p><strong>6. Operationalize</strong></p> <ul> <li><strong>Deliverables</strong>: Finalize and deliver the project.</li> <li><strong>Pilot Project</strong>: Test the model in a real-world scenario.</li> <li><strong>Performance &amp; Constraints</strong>: Monitor and address any constraints.</li> <li><strong>Training</strong>: Educate new users as needed.</li> </ul> <h3 id="key-objectives-of-data-exploration">Key Objectives of Data Exploration</h3> <ul> <li><strong>Understand Data Structure</strong>: Analyze data types, distributions (population vs. sampling), and summary statistics.</li> <li><strong>Assess Data Quality</strong>: Identify missing values, outliers, and duplicates.</li> <li><strong>Identify Patterns and Relationships</strong>: Use correlation to find relationships between variables.</li> <li><strong>Formulate Hypotheses</strong>: Develop hypotheses based on data patterns.</li> </ul> <h3 id="statistical-tools-and-techniques">Statistical Tools and Techniques</h3> <ul> <li><strong>Basic Statistics</strong>: Mean, median, variance, standard deviation, range, interquartile range (IQR).</li> <li><strong>Correlation and Covariance</strong>: Measure relationships between variables (cor(x,y), cov(x,y)).</li> <li><strong>Hypothesis Testing</strong>: Test hypotheses using p-values and confidence intervals. <ul> <li><strong>Two-Sample t-Test</strong>: Compare means of two populations.</li> <li><strong>Welch’s t-Test</strong>: Used when equal variance assumption is not justified.</li> <li><strong>Wilcoxon Rank-Sum Test</strong>: Non-parametric test for non-normal distributions.</li> </ul> </li> </ul> <h3 id="error-types-in-hypothesis-testing">Error Types in Hypothesis Testing</h3> <ul> <li><strong>Type I Error (α)</strong>: Incorrectly rejecting the null hypothesis when it is true.</li> <li><strong>Type II Error (β)</strong>: Failing to reject the null hypothesis when the alternative hypothesis is true. Reduce by increasing sample size.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Effective data exploration is essential for successful data analysis and model building. By understanding data structure, assessing quality, and using appropriate statistical tools, you can make informed decisions and derive actionable insights.</p> <p><a href="/assets/pdf/bigdata/w3_DataPrep.pdf">Lecture 3</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[Methods for Data Exploration]]></summary></entry><entry><title type="html">CSCI971 - Modern Cryptography Week 2</title><link href="https://karangoel59.com/blog/2024/crypto-week2/" rel="alternate" type="text/html" title="CSCI971 - Modern Cryptography Week 2"/><published>2024-08-03T01:09:00+10:00</published><updated>2024-08-03T01:09:00+10:00</updated><id>https://karangoel59.com/blog/2024/crypto-week2</id><content type="html" xml:base="https://karangoel59.com/blog/2024/crypto-week2/"><![CDATA[<p>Modern Crypto</p> <p>shannon perfect cipher</p> <p>message xor key =&gt; cipheher cipheher xor key =&gt; message</p> <p>Perfect Secrecy: A cryptographic system is said to achieve perfect secrecy if the probability distribution of the plaintext, given the ciphertext, is the same as the a priori probability distribution of the plaintext. In simpler terms, even if an attacker has the ciphertext, they gain no additional information about the plaintext than they had before seeing the ciphertext.</p> <p>Pr[M=m]=Pr[M=m∣C=c] This equation means that the probability of any message m being the actual plaintext is the same whether or not the attacker has seen the ciphertext c. Thus, observing the ciphertext provides no additional advantage in guessing the plaintext.</p> <p>Def 2: For every pairs of messages m0, m1 in the message space M, and every ciphertext c in the ciphertext space C That means, the probability that C = c is the same for M = m0 or M = m1</p> <p>We can</p> <p>Theorem (Shannon) In a system with perfect secrecy the number of keys is at least equal to the number of messages</p> <p><a href="/assets/pdf/crypto/2.%20CSCI471971_Cryptographic%20Notions.pdf">Lecture 2</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[Towards Modern Crypto"]]></summary></entry><entry><title type="html">CSCI946 - Big Data Analytics Week 2</title><link href="https://karangoel59.com/blog/2024/big-data-week2/" rel="alternate" type="text/html" title="CSCI946 - Big Data Analytics Week 2"/><published>2024-07-30T01:09:00+10:00</published><updated>2024-07-30T01:09:00+10:00</updated><id>https://karangoel59.com/blog/2024/big-data-week2</id><content type="html" xml:base="https://karangoel59.com/blog/2024/big-data-week2/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In this post, we’ll delve into the data lifecycle, focusing on the different stages, key roles, and common pitfalls associated with managing big data projects. Understanding these elements is crucial for successful data science endeavors.</p> <h2 id="key-stages-of-the-data-lifecycle">Key Stages of the Data Lifecycle</h2> <ol> <li><strong>Discovery</strong> <ul> <li><strong>Domain Understanding</strong>: Comprehend the industry and project objectives.</li> <li><strong>Criteria of Success/Failure</strong>: Define what success looks like and identify potential challenges.</li> <li><strong>Interviews</strong>: Engage with sponsors and stakeholders to gather initial insights.</li> <li><strong>Initial Hypotheses</strong>: Formulate preliminary hypotheses based on the gathered information.</li> </ul> </li> <li><strong>Data Source Identification</strong> <ul> <li>Identify and gather relevant data sources from various departments and warehouses.</li> </ul> </li> <li><strong>Data Preparation</strong> <ul> <li><strong>Sandbox Environment</strong>: Set up a sandbox for data preparation.</li> <li><strong>ETLT (Extract, Transform, Load, Transform)</strong>: Perform data extraction, transformation, and loading. Note that ELT can be slow for large datasets, so ETLT is often preferred.</li> <li><strong>Data Conditioning</strong>: Assess the quality of data, addressing noise, outliers, and missing values.</li> <li><strong>Survey and Visualization</strong>: Use plots such as scatter plots, histograms, and heat maps to understand data distributions and correlations.</li> <li><strong>Scaling and Normalization</strong>: Apply techniques like Z-normalization to standardize data.</li> </ul> </li> <li><strong>Model Planning</strong> <ul> <li><strong>Identify Candidate Models</strong>: Select models based on hypotheses and literature review.</li> <li><strong>Variable Selection</strong>: Choose the relevant variables for modeling.</li> <li><strong>Tools and Languages</strong>: Decide on the tools and programming languages for model development.</li> </ul> </li> <li><strong>Model Building</strong> <ul> <li><strong>Training and Testing</strong>: Separate data for training and testing if necessary to validate model performance.</li> </ul> </li> <li><strong>Communication of Results</strong> <ul> <li><strong>Comparison</strong>: Evaluate the results against success criteria.</li> <li><strong>Reporting</strong>: Clearly communicate the insights and findings.</li> </ul> </li> <li><strong>Operationization</strong> <ul> <li><strong>Implementation</strong>: Finalize and communicate the benefits of the model.</li> <li><strong>Pilot Project</strong>: Test the model in a pilot project to ensure its effectiveness.</li> </ul> </li> </ol> <h2 id="common-mistakes-in-data-science-projects">Common Mistakes in Data Science Projects</h2> <ul> <li><strong>Rushing into Data Collection and Analysis</strong>: Prematurely jumping into analysis without proper planning can lead to suboptimal results.</li> <li><strong>Insufficient Planning</strong>: Not spending adequate time on planning can result in missing critical insights and inefficiencies.</li> </ul> <h2 id="key-roles-in-data-science-projects">Key Roles in Data Science Projects</h2> <ul> <li><strong>Business User</strong>: Provides context and requirements.</li> <li><strong>Project Sponsor</strong>: Supports and funds the project.</li> <li><strong>Project Manager</strong>: Oversees project execution and ensures timelines are met.</li> <li><strong>Business Intelligence Analyst</strong>: Analyzes business data to inform decisions.</li> <li><strong>Database Administrator</strong>: Manages and maintains database systems.</li> <li><strong>Data Engineer</strong>: Develops and maintains data pipelines.</li> <li><strong>Data Scientist</strong>: Designs, implements, and deploys models to derive actionable insights.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Understanding and managing the data lifecycle is essential for successful data science projects. By following a structured approach and avoiding common mistakes, you can ensure that your data projects are effective and deliver valuable insights.</p> <p><a href="/assets/pdf/bigdata/w2-BDLifecycle.pdf">Lecture 2</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[Exploring the key stages and roles in the data lifecycle for effective data science projects.]]></summary></entry><entry><title type="html">CSCI971 - Modern Cryptography Week 1</title><link href="https://karangoel59.com/blog/2024/crypto-week1/" rel="alternate" type="text/html" title="CSCI971 - Modern Cryptography Week 1"/><published>2024-07-27T01:09:00+10:00</published><updated>2024-07-27T01:09:00+10:00</updated><id>https://karangoel59.com/blog/2024/crypto-week1</id><content type="html" xml:base="https://karangoel59.com/blog/2024/crypto-week1/"><![CDATA[<p>Classic Crypto</p> <p>Stream Ciphers</p> <p>random keys</p> <p><a href="/assets/pdf/crypto/1.%20CSCI471971_Introduction.pdf">Lecture 1</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[Introduction"]]></summary></entry><entry><title type="html">CSCI946 - Big Data Analytics Week 1</title><link href="https://karangoel59.com/blog/2024/big-data-week1/" rel="alternate" type="text/html" title="CSCI946 - Big Data Analytics Week 1"/><published>2024-07-23T01:09:00+10:00</published><updated>2024-07-23T01:09:00+10:00</updated><id>https://karangoel59.com/blog/2024/big-data-week1</id><content type="html" xml:base="https://karangoel59.com/blog/2024/big-data-week1/"><![CDATA[<h3 id="sources-of-big-data">Sources of Big Data</h3> <p>Big Data is generated from various sources, including:</p> <ul> <li>Mobile sensors</li> <li>Social media platforms</li> <li>Smart grids</li> <li>Video rendering</li> <li>Medical imaging</li> <li>Genetic data</li> <li>Surveillance cameras</li> <li>Geophysical data</li> </ul> <h3 id="understanding-big-data-its-not-just-about-size">Understanding Big Data: It’s Not Just About Size</h3> <p>Big Data isn’t defined solely by its size; it’s characterized by several key properties, often referred to as the “Vs”:</p> <ul> <li><strong>Velocity</strong>: The speed at which data is generated and processed.</li> <li><strong>Volume</strong>: The sheer amount of data produced.</li> <li><strong>Variety</strong>: The different types of data (structured, semi-structured, unstructured).</li> <li><strong>Value</strong>: The business value that can be derived from the data.</li> <li><strong>Veracity</strong>: The reliability and accuracy of the data.</li> <li><strong>Variability</strong>: The changing nature of data, including inconsistencies and peaks in data flow.</li> </ul> <h3 id="structures-of-big-data">Structures of Big Data</h3> <p>Big Data can be categorized based on its structure, which influences how it is processed and analyzed:</p> <ol> <li><strong>Unstructured Data</strong>: <ul> <li><strong>Description</strong>: Data without a predefined model or organization. It often includes large amounts of text but can also encompass images, videos, and social media content.</li> <li><strong>Examples</strong>: Emails, videos, social media posts, satellite images, presentations.</li> </ul> </li> <li><strong>Quasi-Structured Data</strong>: <ul> <li><strong>Description</strong>: Data with some level of organization but lacking a fixed schema. It often includes metadata or tags to provide a basic structure.</li> <li><strong>Examples</strong>: Web server logs, network logs, event logs.</li> </ul> </li> <li><strong>Semi-Structured Data</strong>: <ul> <li><strong>Description</strong>: Data that doesn’t reside in a traditional database but still has some organizational properties, often using tags or markers to enforce a hierarchical structure.</li> <li><strong>Examples</strong>: XML files, JSON documents, emails (with metadata such as sender, receiver, and date).</li> </ul> </li> <li><strong>Structured Data</strong>: <ul> <li><strong>Description</strong>: Highly organized data, typically stored in databases with a predefined schema. It is arranged into rows and columns, making it easy to enter, query, and analyze.</li> <li><strong>Examples</strong>: Relational databases, spreadsheets, SQL tables.</li> </ul> </li> </ol> <p>This classification is essential for determining the appropriate tools and techniques for processing and analyzing data based on its structure.</p> <h3 id="business-intelligence-vs-data-science">Business Intelligence vs. Data Science</h3> <ul> <li><strong>Business Intelligence (BI)</strong>: Focuses on analyzing and exploring past data to make informed business decisions.</li> <li><strong>Data Science</strong>: Emphasizes predicting future trends and patterns by analyzing current and historical data.</li> </ul> <p><a href="/assets/pdf/bigdata/w1-Introduction.pdf">Lecture 1</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[Introduction to Big Data]]></summary></entry><entry><title type="html">CSCI927 - Service-Oriented Software Engineering Week 3</title><link href="https://karangoel59.com/blog/2024/sose-week3/" rel="alternate" type="text/html" title="CSCI927 - Service-Oriented Software Engineering Week 3"/><published>2024-07-07T18:30:00+10:00</published><updated>2024-07-07T18:30:00+10:00</updated><id>https://karangoel59.com/blog/2024/sose-week3</id><content type="html" xml:base="https://karangoel59.com/blog/2024/sose-week3/"><![CDATA[<p>Business Process Model and Notation (BPMN) is a widely-used standard for representing and analyzing business processes in a graphical format. It bridges the gap between business process design and implementation, making it an essential tool in the toolkit of service-oriented software engineering (SOSE). In this week’s post, we’ll explore the basics of BPMN, its key components, types of activities, events, gateways, and the concept of tokens.</p> <h3 id="what-is-bpmn">What is BPMN?</h3> <p>BPMN stands for Business Process Model and Notation. It is a standardized graphical notation that allows businesses to document, model, and understand their processes in a clear and concise way. BPMN is particularly valuable because it is designed to be understood by all business stakeholders—business analysts, technical developers, and process participants alike.</p> <h3 id="why-bpmn-is-important">Why BPMN is Important</h3> <p>In the realm of SOSE, BPMN plays a crucial role in the design and orchestration of services. By modeling business processes using BPMN, organizations can visualize the flow of activities, identify inefficiencies, and ensure that all aspects of the business process align with organizational goals. This is particularly important in service-oriented environments where multiple services interact with each other in complex ways.</p> <h3 id="key-components-of-bpmn">Key Components of BPMN</h3> <p>BPMN diagrams are composed of several core elements that represent different aspects of a business process. These include:</p> <ul> <li><strong>Flow Objects:</strong> <ul> <li><strong>Events</strong></li> <li><strong>Activities</strong></li> <li><strong>Gateways</strong></li> </ul> </li> <li><strong>Connecting Objects:</strong> <ul> <li><strong>Sequence Flows</strong></li> <li><strong>Message Flows</strong></li> <li><strong>Associations</strong></li> </ul> </li> <li><strong>Swimlanes:</strong> <ul> <li><strong>Pools</strong></li> <li><strong>Lanes</strong></li> </ul> </li> <li><strong>Artifacts:</strong> <ul> <li><strong>Data Objects</strong></li> <li><strong>Groups</strong></li> <li><strong>Annotations</strong></li> </ul> </li> </ul> <h3 id="types-of-activities-tasks-and-subprocesses">Types of Activities: Tasks and Subprocesses</h3> <p>Activities in BPMN represent the work being performed within a business process. They are primarily categorized into tasks and subprocesses.</p> <h4 id="tasks"><strong>Tasks</strong></h4> <p>A task is a basic unit of work that cannot be broken down further within the process context</p> <h4 id="subprocesses"><strong>Subprocesses</strong></h4> <p>A subprocess is a compound activity that encapsulates a set of tasks, providing a way to simplify and manage complex processes.</p> <h3 id="types-of-events-start-intermediate-and-end">Types of Events: Start, Intermediate, and End</h3> <p>Events in BPMN represent things that happen within a business process. These events are divided into three main types:</p> <h4 id="start-events"><strong>Start Events</strong></h4> <p>Start events initiate the process flow. Types include:</p> <ul> <li><strong>None Start Event:</strong> A simple start with no specific trigger.</li> <li><strong>Message Start Event:</strong> Triggered by the receipt of a message.</li> <li><strong>Timer Start Event:</strong> Begins the process at a specific time or after a defined period.</li> <li><strong>Conditional Start Event:</strong> Triggered when a condition becomes true.</li> <li><strong>Signal Start Event:</strong> Responds to a broadcast signal that can trigger multiple processes.</li> </ul> <h4 id="intermediate-events"><strong>Intermediate Events</strong></h4> <p>Intermediate events occur between the start and end events, affecting the flow of the process:</p> <ul> <li><strong>Catch Event:</strong> Waits for a trigger (e.g., message, timer, signal) before proceeding.</li> <li><strong>Throw Event:</strong> Generates a trigger that can influence the process or other processes.</li> <li><strong>Timer Intermediate Event:</strong> Introduces a delay or specifies timing within the process.</li> <li><strong>Message Intermediate Event:</strong> Indicates a point where the process sends or receives a message.</li> <li><strong>Error Intermediate Event:</strong> Handles errors that occur during the process.</li> <li><strong>Escalation Event:</strong> Used to indicate an escalation in a process, typically signaling a higher priority action.</li> </ul> <h4 id="end-events"><strong>End Events</strong></h4> <p>End events signify the completion of the process. Types include:</p> <ul> <li><strong>None End Event:</strong> Marks the simple end of a process.</li> <li><strong>Message End Event:</strong> Sends a final message when the process ends.</li> <li><strong>Error End Event:</strong> Indicates that the process ended due to an error.</li> <li><strong>Terminate End Event:</strong> Ends the entire process immediately, regardless of other ongoing activities.</li> <li><strong>Signal End Event:</strong> Broadcasts a signal that other processes can respond to.</li> </ul> <h3 id="types-of-gateways">Types of Gateways</h3> <p>Gateways control the divergence and convergence of the process flow. They represent decision points, where the process can take multiple paths. Types of gateways include:</p> <ul> <li><strong>Exclusive Gateway (XOR):</strong> Allows only one of several paths to be taken, based on conditions.</li> <li><strong>Parallel Gateway (AND):</strong> Allows multiple paths to be taken simultaneously.</li> <li><strong>Inclusive Gateway (OR):</strong> One or more paths can be taken based on conditions.</li> <li><strong>Event-Based Gateway:</strong> Directs the flow based on an event that occurs.</li> <li><strong>Complex Gateway:</strong> Combines multiple types of decision criteria, allowing for more advanced control.</li> </ul> <h3 id="the-concept-of-tokens-in-bpmn">The Concept of Tokens in BPMN</h3> <p>In BPMN, the concept of a token is used to represent the flow of control within a process. A token is an abstract representation of a process instance moving through the model. When a process is initiated, a token is created and it moves through the sequence flows, activities, and gateways.</p> <ul> <li><strong>Token and Activities:</strong> As the token moves through an activity, it represents the execution of that activity.</li> <li><strong>Token and Gateways:</strong> When a token reaches a gateway, the type of gateway determines how the token will proceed. For example, an exclusive gateway will allow the token to pass through only one of the available paths, while a parallel gateway will split the token into multiple tokens, one for each path.</li> </ul> <p>The movement of tokens through a BPMN diagram helps visualize how a process flows from start to end, including how decisions and parallel activities are handled.</p> <h3 id="bpmn-in-action">BPMN in Action</h3> <p>Let’s consider a simple example: an order processing system in an online store. Using BPMN, you could model the process starting from the customer placing an order, followed by order validation, payment processing, and finally, order fulfillment and delivery.</p> <ol> <li><strong>Start Event:</strong> The process begins with the customer placing an order.</li> <li><strong>Task:</strong> The system validates the order to ensure all information is correct.</li> <li><strong>Gateway:</strong> A decision is made whether the payment is successful.</li> <li><strong>Task:</strong> If successful, the order is sent to the warehouse for fulfillment.</li> <li><strong>End Event:</strong> The process concludes when the order is delivered to the customer.</li> </ol> <p>This simple diagram provides a clear and visual representation of the order process, making it easy to understand and analyze.</p> <h3 id="bpmn-and-sose">BPMN and SOSE</h3> <p>In SOSE, BPMN can be used to model the interactions between various services within a system. For example, in a service-oriented architecture, you might have services for customer management, order processing, inventory management, and shipping. BPMN allows you to model how these services interact, where the dependencies are, and how information flows between them.</p> <p>This not only helps in the design phase but also in monitoring and optimizing the processes once they are implemented. By using BPMN, you can ensure that your service architecture is aligned with business processes and is capable of adapting to changes in business needs.</p> <p><a href="/assets/pdf/sose/3.Business%20Process%20Modelling%20and%20Management.pdf">Lecture 3</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[BPMN]]></summary></entry><entry><title type="html">CSCI935 - Computer Vision Algorithms and Systems</title><link href="https://karangoel59.com/blog/2024/computer-vision/" rel="alternate" type="text/html" title="CSCI935 - Computer Vision Algorithms and Systems"/><published>2024-07-05T04:00:01+10:00</published><updated>2024-07-05T04:00:01+10:00</updated><id>https://karangoel59.com/blog/2024/computer-vision</id><content type="html" xml:base="https://karangoel59.com/blog/2024/computer-vision/"><![CDATA[<h3 id="my-learning-from-computer-vision-algorithms">My Learning from Computer Vision Algorithms</h3> <p>As I progress through my journey in the Computer Vision Algorithms subject, I’ve come to appreciate the intricate details that shape how machines perceive and interpret visual data. This subject has offered me a comprehensive understanding of how light, color, and various algorithms work together to enable computers to perform tasks that once seemed exclusive to human vision. Here’s a breakdown of some key concepts and techniques I’ve learned:</p> <h4 id="understanding-light-and-color"><strong>Understanding Light and Color</strong></h4> <p>Light is more than what we see; it’s electromagnetic radiation that interacts with our eyes, triggering processes that result in color perception. Human eyes contain rods for low-light vision and cones that detect color. The cones come in three types, each responding to different wavelengths of light, forming the basis of the <strong>tristimulus theory</strong>. This theory suggests that any color can be represented using three values corresponding to red, green, and blue (RGB). This understanding is crucial in computer vision, where <strong>color models</strong> like RGB, CIE XYZ, and CIELab are fundamental to image processing.</p> <h4 id="the-mechanics-of-image-sensors"><strong>The Mechanics of Image Sensors</strong></h4> <p>In exploring how images are captured, I learned about <strong>image sensors</strong> like CMOS and CCD, which convert light into electric charges and then into digitized image samples. This digitization process involves converting continuous signals into quantized samples, with proper sampling rates essential to avoid aliasing.</p> <p>Digital cameras use <strong>color filter arrays (CFA)</strong> to capture accurate color information by representing the real world through three color channels—red, green, and blue. <strong>Gamma correction</strong> maps these quantized samples into a domain that our eyes perceive as uniform, enhancing digital image quality.</p> <h4 id="the-concept-of-machine-vision"><strong>The Concept of Machine Vision</strong></h4> <p><strong>Machine vision</strong> involves a multistage process where each stage influences the next. This concept highlights the importance of <strong>image enhancement</strong> to make images suitable for specific applications. However, image capture often introduces <strong>distortions</strong> that degrade quality, requiring quantitative metrics to ensure accuracy.</p> <p><img src="/assets/img/T3-Image-Quality-and-Enhancement-pdf.png" alt="Machine Vision" width="500"/></p> <p><strong>Noise</strong> is a challenge in digital imaging, arising from the analog components in cameras. It requires statistical methods for reduction to preserve useful information.</p> <h4 id="edge-detection-and-keypoint-detection"><strong>Edge Detection and Keypoint Detection</strong></h4> <p>Edge detection is vital in computer vision, as edges indicate boundaries and shapes. <strong>Gradient calculation</strong> detects these discontinuities in images. However, noise can affect edge detection, necessitating noise reduction techniques.</p> <p>The <strong>Canny edge detector</strong> is a robust method combining edge detection, thinning, tracing, and linking to produce clean edges. <strong>Keypoint detection</strong> follows, using algorithms like the <strong>Harris Corner Detector</strong> to identify points of interest in an image. The Harris Detector’s mathematical foundation, including eigenvalue analysis, ensures reliable keypoint detection.</p> <h4 id="advanced-techniques-sift-and-hough-transform"><strong>Advanced Techniques: SIFT and Hough Transform</strong></h4> <p>The <strong>SIFT (Scale-Invariant Feature Transform)</strong> algorithm is crucial for detecting, localizing, and describing keypoints, enabling robust object recognition across varying scales and orientations.</p> <p>The <strong>Hough transform</strong> locates shapes like lines and circles within an image. It converts the image into a binary edge map, generates shape parameters for each edge point, and accumulates these parameters in an array representing the parameter space. Peaks in this array correspond to likely shapes, facilitating shape localization.</p> <h4 id="segmentation-and-object-detection"><strong>Segmentation and Object Detection</strong></h4> <p><strong>Segmentation</strong> is essential for object detection, involving methods like clustering-based approaches, thresholding, and <strong>K-means clustering</strong>. Advanced techniques like <strong>mean shift clustering</strong> and <strong>normalized cuts (NCut)</strong> provide more precise segmentation in complex scenes.</p> <p><strong>Object detection</strong> involves identifying and localizing objects within images. Techniques such as <strong>face detection</strong> and <strong>pedestrian detection</strong> use specific features and classifiers to recognize and categorize objects, crucial for applications like surveillance and autonomous driving.</p> <h4 id="motion-estimation-and-optical-flow"><strong>Motion Estimation and Optical Flow</strong></h4> <p><strong>Motion estimation</strong> determines the movement of objects within image sequences. <strong>Optical flow</strong> calculates the apparent motion of brightness patterns across frames. Techniques like <strong>Horn and Schunck</strong> and <strong>Lucas and Kanade</strong> methods estimate optical flow, each offering different approaches to handling motion.</p> <h3 id="what-i-found-most-interesting-while-learning-about-computer-vision-algorithms">What I Found Most Interesting While Learning About Computer Vision Algorithms</h3> <p>Studying computer vision algorithms has been enlightening, revealing fascinating aspects of technology and human perception. Key points of appreciation include:</p> <h4 id="1-the-intricacies-of-human-vision"><strong>1. The Intricacies of Human Vision</strong></h4> <p>The complexity and capability of human vision are remarkable. I was amazed to learn that our eyes can perceive a spectrum of colors that digital devices cannot fully reproduce. The CIE 1931 color space models human color perception, highlighting the sophistication of our visual system compared to technology.</p> <h4 id="2-the-mathematical-foundations-of-computer-vision"><strong>2. The Mathematical Foundations of Computer Vision</strong></h4> <p>The extensive mathematical framework underlying computer vision is intriguing. Concepts like Gaussian distributions, the Central Limit Theorem, and the Fourier Transform illustrate how images can be analyzed as signals through mathematical lenses. This approach connects computer vision to broader areas of signal processing and applied mathematics.</p> <h3 id="what-i-dont-like">What I Don’t Like</h3> <p>While foundational formulas and algorithms are crucial, many have become somewhat redundant with the rise of deep learning and Convolutional Neural Networks (CNNs). Modern approaches have simplified and automated tasks that were once complex and algorithm-intensive. Consequently, some traditional methods and mathematical formulations have become less prominent, shifting the focus toward leveraging deep learning models for advanced and efficient image processing.</p> <h3 id="overall-experience">Overall Experience</h3> <p>I achieved a Distinction in the course. I feel significantly more knowledgeable about computer vision than before and am eager to apply this knowledge in practice.</p> <p><a href="/assets/pdf/cv/Lectures.zip">Lectures</a> <a href="/assets/pdf/cv/Assignment1.zip">Assignment 1</a> <a href="/assets/pdf/cv/Assignment2.zip">Assignment 2</a> <a href="/assets/pdf/cv/Assignment3.zip">Assignment 3</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[My Learning on CSCI935]]></summary></entry><entry><title type="html">CSIT882 - Data Management Systems</title><link href="https://karangoel59.com/blog/2024/database/" rel="alternate" type="text/html" title="CSIT882 - Data Management Systems"/><published>2024-07-05T04:00:01+10:00</published><updated>2024-07-05T04:00:01+10:00</updated><id>https://karangoel59.com/blog/2024/database</id><content type="html" xml:base="https://karangoel59.com/blog/2024/database/"><![CDATA[<h2 id="csit882---data-management-systems">CSIT882 - Data Management Systems</h2> <p>This subject proved to be relatively straightforward, covering fundamental concepts in data management without introducing anything particularly groundbreaking. Below, I have provided a brief overview of the key topics covered in the lecture notes.</p> <ol> <li><strong>00Overview.pdf</strong>: Summarizes the key topics and objectives of the course.</li> <li><strong>01Introduction.pdf</strong>: Introduces fundamental database concepts and their significance.</li> <li><strong>01graphdatamodels.pdf</strong>: Discusses graph data models for representing data as nodes and edges, useful for applications like social networks.</li> <li><strong>02DatabaseDesign.pdf</strong>: Covers principles for designing databases, including schema creation and data organization.</li> <li><strong>02ERmodels.pdf</strong>: Focuses on Entity-Relationship (ER) models for visualizing data and relationships.</li> <li><strong>03advancedobjectmodeling.pdf</strong>: Explores advanced techniques in object modeling beyond basic object-oriented concepts.</li> <li><strong>03objectdatamodel.pdf</strong>: Examines object data models integrating object-oriented programming concepts into databases.</li> <li><strong>04conceptualModeling.pdf</strong>: Addresses conceptual modeling techniques for abstract database representations.</li> <li><strong>04designpatterns.pdf</strong>: Discusses design patterns for solving common database design problems.</li> <li><strong>05FunctionalDependencies.pdf</strong>: Explains functional dependencies and their impact on database structure.</li> <li><strong>05relationaldatamodel.pdf</strong>: Covers the relational data model, organizing data into tables (relations) with rows and columns.</li> <li><strong>06DerivationsOfFunctionalDependencies.pdf</strong>: Looks into functional dependencies’ derivations and their effects on database normalization.</li> <li><strong>06logicaldesign.pdf</strong>: Focuses on translating conceptual models into logical schemas.</li> <li><strong>07DatabaseNormalization_1.pdf</strong>: Introduces database normalization to reduce redundancy and improve data integrity.</li> <li><strong>07databasedesignquality.pdf</strong>: Discusses criteria for high-quality database design, including efficiency and maintainability.</li> <li><strong>08DatabaseNormalization_2.pdf</strong>: Continues the normalization discussion with additional normal forms.</li> <li><strong>08introductiontosql.pdf</strong>: Introduces SQL (Structured Query Language) for querying and manipulating relational databases.</li> <li><strong>09IntroductionToTransactionProcessing_1.pdf</strong>: Explains transaction processing basics, including ACID properties and concurrency control.</li> <li><strong>09sqlddl.pdf</strong>: Covers SQL Data Definition Language (DDL) commands for defining and modifying database structures.</li> <li><strong>10IntroductionToTransactionProcessing_2.pdf</strong>: Delves into advanced transaction processing aspects, including recovery and management.</li> <li><strong>10sqldml.pdf</strong>: Focuses on SQL Data Manipulation Language (DML) commands for querying and manipulating data.</li> <li><strong>11demoRollbackCommitTransaction.pdf</strong>: Demonstrates SQL commands for rolling back and committing transactions.</li> <li><strong>12TransactionProcessingInSQL (1).pdf</strong>: Explores transaction processing specifics in SQL.</li> <li><strong>12TransactionProcessingInSQL.pdf</strong>: Continues the topic of transaction processing with further examples and explanations.</li> <li><strong>12select-2.pdf</strong>: Discusses advanced SQL SELECT statements, including complex queries and data retrieval.</li> <li><strong>13NoSQL.pdf</strong>: Introduces NoSQL databases and their models, including document, key-value, column, and graph databases.</li> <li><strong>13select-3.pdf</strong>: Covers additional advanced SQL SELECT queries, including joins, subqueries, and set operations.</li> <li><strong>14select-4.pdf</strong>: Continues advanced SQL SELECT topics with more examples and use cases.</li> <li><strong>15select-5.pdf</strong>: Further explores advanced SQL SELECT statements and techniques for efficient data retrieval.</li> <li><strong>16views.pdf</strong>: Discusses SQL views, which are virtual tables created from queries to simplify complex queries and manage data security.</li> <li><strong>17advancedddlanddml.pdf</strong>: Examines advanced DDL and DML concepts, including complex database definitions and manipulation techniques.</li> <li><strong>18archreldbserver.pdf</strong>: Covers the architecture of relational database servers, including query processors and storage managers.</li> <li><strong>19dbsecurity.pdf</strong>: Discusses database security measures to protect data from unauthorized access and breaches.</li> <li><strong>20discraccesscontrol.pdf</strong>: Focuses on discretionary access control, granting permissions based on user roles.</li> <li><strong>21usermanagement.pdf</strong>: Covers user management, including creating and managing user accounts and permissions.</li> <li><strong>22dataintegrity.pdf</strong>: Discusses data integrity to ensure accuracy and consistency throughout data lifecycle.</li> <li><strong>23datavulnerabilities.pdf</strong>: Examines vulnerabilities in data management systems and common threats.</li> <li><strong>24databaseauditing.pdf</strong>: Explores database auditing practices for monitoring and recording activities.</li> <li><strong>25dataprivacy.pdf</strong>: Focuses on data privacy concerns and regulations to protect personal and sensitive data.</li> <li><strong>26legalethicaldatamanagement.pdf</strong>: Discusses legal and ethical considerations in data management, including compliance with laws and ethical standards.</li> </ol> <h3 id="overall-review">Overall Review</h3> <p>I was able to achieve a distinction with relatively little effort. Although the course was not particularly challenging, it provided valuable insights, particularly into object modeling.</p> <p><a href="/assets/pdf/database/DB.zip">Lectures</a> <a href="/assets/pdf/database/Assign.zip">Assignments</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[My Learning on CSIT882]]></summary></entry><entry><title type="html">CSCI933 - Machine Learning Algorithms and Applications</title><link href="https://karangoel59.com/blog/2024/machine-learning/" rel="alternate" type="text/html" title="CSCI933 - Machine Learning Algorithms and Applications"/><published>2024-07-05T04:00:01+10:00</published><updated>2024-07-05T04:00:01+10:00</updated><id>https://karangoel59.com/blog/2024/machine-learning</id><content type="html" xml:base="https://karangoel59.com/blog/2024/machine-learning/"><![CDATA[<h2 id="csci933---machine-learning-algorithms-and-applications">CSCI933 - Machine Learning Algorithms and Applications</h2> <p>In the course CSCI933, we explored various facets of machine learning, covering theoretical foundations and practical applications. Here’s a detailed look at what we covered:</p> <h3 id="introduction-to-linear-algebra">Introduction to Linear Algebra</h3> <p>Linear algebra is foundational for understanding machine learning algorithms. The course introduced vectors and matrices, essential tools for modeling and solving complex problems. Key concepts included:</p> <ul> <li><strong>Vectors and Matrices</strong>: Understanding their definitions, transpositions, and practical applications in representing and manipulating data.</li> <li><strong>Matrix Algebra</strong>: Delved into operations like matrix multiplication and addition, and properties such as symmetry, skew-symmetry, and orthogonality.</li> <li><strong>Applications</strong>: Principal Component Analysis (PCA) for dimensionality reduction, utilizing Singular Value Decomposition (SVD) for matrix factorization.</li> </ul> <h3 id="probability-theory">Probability Theory</h3> <p>Probability theory equips us to model and reason about uncertainty in data. Key topics included:</p> <ul> <li><strong>Events and Probabilities</strong>: Events are subsets of a sample space, with probabilities assigned to these events.</li> <li><strong>Conditional Probability</strong>: The probability of an event given that another event has occurred.</li> <li><strong>Random Variables</strong>: Variables that assign values to events in a probability space, including concepts like expected value and variance.</li> </ul> <h3 id="introduction-to-machine-learning">Introduction to Machine Learning</h3> <p>Machine learning involves extracting patterns from data to make predictions or decisions. We covered:</p> <ul> <li><strong>Common Scenarios</strong>: Supervised, unsupervised, semi-supervised, and reinforcement learning, among others.</li> <li><strong>Tasks</strong>: Classification, regression, clustering, dimensionality reduction, natural language understanding, and more.</li> <li><strong>Performance and Experience</strong>: Measures of performance like accuracy and error rate, and the impact of supervised vs. unsupervised experiences on algorithm performance.</li> </ul> <h3 id="regression-techniques">Regression Techniques</h3> <p>Regression is a key supervised learning method for prediction. Topics included:</p> <ul> <li><strong>Introduction to Regression</strong>: Historical context and its evolution.</li> <li><strong>Types of Regression</strong>: Linear regression, kernel ridge regression, lasso regression, and elastic net regression.</li> <li><strong>Regularization</strong>: Techniques to prevent overfitting, such as L1 (Lasso), L2 (Ridge), and elastic net regularization. Regularization helps improve model generalization by adding penalty terms to the loss function.</li> </ul> <h3 id="classification-and-pattern-recognition">Classification and Pattern Recognition</h3> <p>Classification involves designing models to recognize and categorize patterns. Key points:</p> <ul> <li><strong>Pattern Recognition</strong>: Designing machines to classify objects based on features.</li> <li><strong>Classifier Design</strong>: Specifying model parameters to ensure optimal performance.</li> <li><strong>Bayes Decision Rule</strong>: A probabilistic method for classification that minimizes error or risk by choosing the class with the highest posterior probability.</li> </ul> <h3 id="neural-networks">Neural Networks</h3> <p>Neural networks mimic the brain’s functioning for various tasks. Topics covered:</p> <ul> <li><strong>Introduction to Neural Networks</strong>: Modeling the brain’s task performance using synaptic weights.</li> <li><strong>Neuron Models</strong>: Basic components of a neuron, including synapses, adder, and activation functions.</li> <li><strong>Activation Functions</strong>: Common functions such as Threshold, Logistic (Sigmoid), ReLU, and Softmax.</li> <li><strong>Network Architectures</strong>: Single Layer, Multilayer Feedforward, and Recurrent Networks.</li> </ul> <h3 id="advanced-neural-networks">Advanced Neural Networks</h3> <p>We explored specialized neural network architectures:</p> <ul> <li><strong>Perceptron</strong>: A simple model classifying inputs by creating a hyperplane as a decision boundary.</li> <li><strong>Backpropagation Algorithm</strong>: A method for training multilayer perceptrons using forward and backward passes to update weights.</li> </ul> <h3 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h3> <p>CNNs are designed for processing grid-like data:</p> <ul> <li><strong>CNN Architecture</strong>: Includes convolutional layers, subsampling (pooling), and feature mapping.</li> <li><strong>Popular Architectures</strong>: LeNet-5, AlexNet, GoogLeNet, and ResNet.</li> </ul> <h3 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h3> <p>RNNs handle sequential data:</p> <ul> <li><strong>Time Series Data</strong>: Applications in stock prices, electricity consumption, and more.</li> <li><strong>RNNs</strong>: Processing sequences by maintaining an internal state.</li> <li><strong>LSTM and GRU</strong>: Advanced RNNs addressing limitations with their unique computational structures.</li> </ul> <h3 id="language-modelling">Language Modelling</h3> <p>Language modeling is crucial for understanding and generating human language:</p> <ul> <li><strong>Text Encoding</strong>: Techniques like one-hot encoding and word embeddings.</li> <li><strong>Neural Machine Translation (NMT)</strong>: Models for translating text using encoder-decoder architectures.</li> <li><strong>Transformer</strong>: A powerful model utilizing multi-head attention and positional embeddings for NLP tasks.</li> </ul> <h3 id="regularization-and-model-improvement">Regularization and Model Improvement</h3> <p>Regularization techniques enhance model performance:</p> <ul> <li><strong>Regularization Methods</strong>: Includes data transformation, network architecture adjustments, and optimization techniques.</li> </ul> <h3 id="autoencoders-and-gans">Autoencoders and GANs</h3> <p>Advanced neural network models:</p> <ul> <li><strong>Autoencoders</strong>: Networks trained to reconstruct inputs.</li> <li><strong>Generative Adversarial Networks (GANs)</strong>: Two networks (generator and discriminator) trained together to produce synthetic data.</li> </ul> <h3 id="graphs-and-graph-neural-networks-gnns">Graphs and Graph Neural Networks (GNNs)</h3> <p>Graphs represent complex relationships:</p> <ul> <li><strong>Graphs</strong>: Definitions and types (undirected, directed, weighted, bipartite).</li> <li><strong>Graph Neural Networks</strong>: Techniques for training GNNs, including message passing and node feature updates.</li> </ul> <h3 id="overall-review">Overall Review</h3> <p>My review of this subject is mixed. Although I feel there is still a lot to learn in machine learning, I no longer see it as an untamable beast. Neural networks are just function approximators, and at their core, everything boils down to linear algebra. The journey towards creating conscious artificial intelligence remains uncertain—maybe it will never be achieved; who knows?</p> <p>On the practical side, I feel capable of implementing machine learning models effectively. However, I don’t yet have a deep grasp of the theory behind each model. I understand their functionalities and applications, but the mathematical intricacies of how they work remain elusive. Questions about the underlying mathematics are still lingering in my mind.</p> <p>I scored 70% in the course, and the theoretical aspects were particularly challenging. SHOULD HAVE FOCUSED MORE ON LECTURES</p> <p><a href="/assets/pdf/ml/ml.zip">Lectures</a> <a href="/assets/pdf/ml/Yann_LeCun__A_reflection_on_building_machines_with_human_intelligence.pdf">Assign1</a> <a href="/assets/pdf/ml/Intrusion_detection_using_logistic_regression__A_comparison_of_regularization_methods_and_PCA_logistic_regression_method.pdf">Assign2</a> <a href="/assets/pdf/ml/Aspect_based_sentiment_analysis__A_study_of_the_IMDB_review_database_group_at.pdf">Assign3</a></p>]]></content><author><name></name></author><category term="learning"/><category term="projects"/><category term="learning"/><category term="uow"/><summary type="html"><![CDATA[A comprehensive overview of CSCI933, covering key machine learning algorithms and their applications.]]></summary></entry></feed>