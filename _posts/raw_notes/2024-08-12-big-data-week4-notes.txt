---

layout: post  
title: "CSCI946 - Big Data Analytics Week 3"  
date: 2024-08-12 15:09:00  
description: "Clustering"  
tags: projects learning uow
categories: learning  
giscus_comments: true  
featured: false  

---

## Hypothesis Testing

Null hypothesis (H0) vs Alternative hypothesis (HA)

Mean can't tell difference, we have to use variance

Student’s t-test
– Assumptions: Two populations, normally distributed and have a similar 
variance.

Welch’s t-test
– Assumptions: Two populations, normally distributed (Gaussian distribution).

Wilcoxon Rank-Sum Test
– Assumptions Two populations, not normal distributed.

ANOVA
– When: More than two populations.


ANOVA (Analysis of variance)
Multiple t test not well

ANOVA tests if any of the population means differ
from the other population means
population is assumed to be normal and have the same variance

H0 = all mean same
HA = atlest one pair is difference

Compute F test Statistic

– Between-groups mean sum of squares - variability of group means around the overall mean
– Within-groups mean sum of squares - variability within each group

F = SB/SW

The larger the F-test statistic, the greater the 
likelihood that the difference of means are due to 
something other than chance alone

One-Way ANOVA - one independent factor
Two-Way ANOVA - two independent factor

Limitiation
Data Should be assumed normal distributed
Sensitive of Outliers
Do not tell which pair is difference


Need to HSD (Honest Significant Difference) for futher Analysis

– Calculation of the HSD
• Critical value (value that is statistically significant) from studentized range distribution, Mean square within 
groups(from ANOVA), number of groups


– Decision Rule
• For each pair of means, calculate the absolute difference.
• Compare the absolute difference to the HSD value.
• If the absolute difference is greater than the HSD, the pair of means is 
considered significantly different


Clustering Analysis

K-means, DBSCAN, SOM 

note - they are hard clutering algo, not like lda which can be consider soft clustering
note - they are run on unsupervised data, unlike classification which run on supervised data (KNearest, Des Tree, SVM)

K mean
each object is a point in a n dimension space

chosen value of k, identify k clusters of 
objects based on the objects’ proximity 

K mean can be used 
image processing
medical
even for data compression


steps
choose value of k, create k centeroids (random intit)
compute distance of each data points for centriods, assign each point to closest centriods
Update centroids of each cluster

the distance are computed using euclidean distance
centroid of cluster is computed like center of gravity


Way to choose k
heuristic - if you have
Sum of squares , min J (Find the elbow)

principle 
If using more clusters does not better distinguish 
the groups, it is almost certainly better to go with 
fewer clusters

Which attribute to include
Object attributes
highly correlated attributes (can be identitified cor matrix, scatter plot)
Feature selection, PCA, etc. 

Units of measure could affect clustering result
Rescaling attributes affect clustering result
– Divide each attribute by its standard deviation 
– Normalisation: mean=0, sdev=1, particularly when 
Euclidean distance is used

• K-means clustering is easily applied to numeric data where 
the concept of distance can naturally be applied
• K-modes handles categorical data


Sometimes it is better to convert categorial (or 
symbolic) data to numerical i.e. {hot, warm, cold} to {1,0,-
1}, or use one-hot encoding (downside increase dimension).

K mean downside

sensitive to noise
cluster vary in density
clusters differ significantly in size
cluster can be empty
sensitive to starting position (run it Multiple time)



Density based clustering

Density-based clustering locates regions of high 
density that are separated from one another by 
regions of low density.

DBSCAN
 Given a density threshold (MinPts) and a radius (Eps), 
the points in a dataset are classified into three types: 
core point, border point, and noise point.

Core points: Whose density >= MinPts
 border point is not a core point but falls within the 
neighborhood of a core point

A noise point is any point that is neither a core point 
nor a border point.

 Steps of DBSCAN to identify clusters
– Step 1: Label each point as either core, 
border, or noise point.
– Step 2: Mark each group of Eps connected 
core points as a separate cluster.
– Step 3: Assign each border point to one of 
the clusters of its associate core points.

• DBSCAN:
– Resistant to noise and outliers
– Can handle clusters of different shapes and sizes
– Computational complexity is similar to K-means
• When DBSCAN does not work well
– Varying densities
• Can be overcome by using sampling
– Sparse and high-dimensional data
• Can be overcome by using topology preserving dimension 
reduction techniques



Self-Organizing Maps (used for clustering and visulization)

type of Neural Network (NN)
Unsupervised algorithm
Project high dimensional data onto a n dimensional display space (mostly 2d)
Topology preserving - two data points close, will remain close in Maps

Design
Each nueron is like a one cluster in k mean
arrange in grid
each neuron have a weight (like centeroid) 

 The weights in a SOM are trained in a two step algorithm:
– Step 1: Competitive step
• Every neuron is examined to calculate which one's 
weights is most similar to the input vector. The winning 
neuron is known as the Best Matching Unit (BMU).
– Step 2: Cooperative step
• The weights of the BMU and the weights of the 
neighboring neurons is updated.

SOMs are an excellent choice for data visualization
• Many visualization techniques
– From exploratory data analytics
– Dimension reduction techniques
• i.e. PCA, t-SNE, SOM,…
• Why use Self-Organizing Maps (SOMs) in BDA?
– Topology preservation (unlike PCA)
– Able to deal with new data & missing values (unlike t-SNE)
– Can reduces the amount of information that needs to be 
evaluated 
– Produces prototypes that represent the full set of attributes 
with their original meaning (unlike PCA

When not to use SOMs in BDA:
–When the data is very sparse
–When cardinality (limited resolution) of the 
map is a problem.
–When multi-core compute infrastructure is 
unavailable

Hierarchical Clusterin

– Hierarchical agglomerative clustering
– Hierarchical divisive clusterin